We are interested in the problem of fitting a time series such that it remains \textit{constant} over some unknown periods of time. 
Here the notion of constant is dictated by the dynamics of the system, which we will assume is governed by a series of continuous or discrete differential equation. 

Consider observations
\begin{equation}
    y_i \sim N(x_i, \sigma_i^2) \qquad i=1,2,\ldots, N
\end{equation}
where $x_i = x(t_i; \theta) \in \R^n$ are some latent variables with observation times $t_i$ following some \textbf{continuous} dynamics given by the differential equation
\begin{equation}
    \frac{dx}{dt}
    = 
    f(x; \theta(t), t),
    \qquad 
    x(t_0) = x_0
    \label{eq:ode}
\end{equation}
where $\theta = \theta(t) \in \R^p$ is a continuous parameter of the system. 
Notice that we had emphasized the potential time dependency of the parameter $\theta$ as a function of time. 
Our goal is to find $\theta(t)$ such that the model approximates well enough the observations $y_i$ at the same time we impose the constraint that $\theta(t)$ is a piecewise constant function. 
This dynamics results adequate for physical systems that evolve following some predictated dynamics until an abrupt episode changes the default configuration of the system, making the system to evolve following a new value of the system parameter $\theta(t)$. 

A different description of the dynamics can be given with a set of \textbf{discrete} equations. 
This can arise by directly discretizing the continuous differential equation using a numerical solver, or just by describing the system as a discrete time series. 
In both cases, latent variables are generated by the equation 
\begin{equation}
    x_{i+1} = f_i(x_i, \theta_i),
\end{equation}
where $f_i$ is an evolution function. 
For many cases, this function will be linear and can be directly be described as 
\begin{equation}
    x_{i+1} = A_i(\theta_i) \, x_i + b_i(\theta_i)
    \label{eq:}
\end{equation}
with $A_i \in \R^{n \times n}$ and $b \in \R^n$.

A first approach to solve this problem is by to directly solve the continuous minimization problem 
\begin{equation}
    \min_{\theta(\cdot)}
    \sum_{i=1}^N
    \| y_i - \text{ODESolve}(t_i; \theta(\cdot)) \|_2^2
    + 
    \lambda 
    \int \left \| \frac{d\theta}{dt}(\tau) \right \|_2 d\tau
\end{equation}
where $\text{ODESolve}(t; \theta(\cdot))$ is the numerical solution of the differential equation \eqref{eq:ode} evaluated at time $t$ with time dependent parameter $\theta(\cdot)$.
The second term penalizes changes in the function $\theta(\cdot)$, encouraging functions that stay constant over long period of times. 
Solving such optimization problem will require to be able to compute the gradient of the numerical solution with respect to the parameter $\theta$. 
Furthermore, we need to parametrize the space of possible functions $\theta(\cdot)$.

A second approach consists in working directly with the discrete dynamical system and solve the problem\footnote{Notice that in this case we have the problem of what to do with the location of the changepoints, since changes in the value of the parameter $\theta$ don't necessary have to coincide with the $t_i$. We can instead consider a similar loss function but where the empirical error just evalu ates in a subset of indices $I \subset \{1 , 2, \ldots, N \}$ where we actually have observations, leaving the $1,2, \ldots, N$ for the latent variables where the solution is evaluated.}
\begin{equation}
    \min_{\theta_1, \theta_2, \ldots, \theta_N}
    \sum_{i=1}^N
    \| y_i - x_i \|_2^2
    + 
    \lambda
    \sum_{i=1}^{N-1}
    \| \theta_{i+1} - \theta_i \|_2
\end{equation}
subject to the constraint 